tokenizer: 
    en: /apdcephfs/share_916081/johntianlan/gpt2_english
    zh: /apdcephfs/share_916081/johntianlan/gpt2-chinese-cluecorpussmall
pretrained_model: 
    en: /apdcephfs/share_916081/johntianlan/gpt2_english
    zh: /apdcephfs/share_916081/johntianlan/gpt2-chinese-cluecorpussmall

# test configuration
# lambda and alpha hyper-parameters come from retro paper: https://arxiv.org/pdf/2112.04426.pdf
# lambda=0 and alpha=1.0 will make knn-lm degenerates to gpt2 model
test:
    seed: 0
    batch_size: 1
    max_len: 512
    search_topk: 1024
    temp: 1.0
    lambda: 0.118
    alpha: 0.00785
    ppl_max_len: 200

# infernece
inference:
    seed: 0
    batch_size: 64
    index_type: IVF500000,PQ16
    index_nprobe: 64
    dimension: 768
    max_len: 256
    ppl_max_len: 200
